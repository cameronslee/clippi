{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Preprocessing\n",
    "\n",
    "Features:\n",
    "- Zero shot classification (what best describes what is happening in this video during sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path already exists\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def perror(msg):\n",
    "    print(\"error: \" + msg)\n",
    "\n",
    "def mkdir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        return path\n",
    "\n",
    "    print(\"path already exists\")\n",
    "    return\n",
    "\n",
    "VIDEO_URL = \"https://www.youtube.com/watch?v=33pBg_UaeJk\"\n",
    "\n",
    "OUTPUT_DIR = 'cache/'  # target output for preoprocessing is cache\n",
    "extracted_id = VIDEO_URL.split(\"/\")[-1]\n",
    "\n",
    "mkdir(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytube import YouTube\n",
    "\n",
    "yt = YouTube(VIDEO_URL)\n",
    "yt.streams.filter(progressive=True, file_extension='mp4').order_by(\n",
    "    'resolution').desc().first().download(output_path=OUTPUT_DIR)\n",
    "\n",
    "extracted_title = yt.streams[0].default_filename\n",
    "OUTPUT_FILE = OUTPUT_DIR + extracted_title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import av\n",
    "import numpy as np\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "np.random.seed(0)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(columns=['start_frame', 'end_frame', 'start_time', 'end_time'])\n",
    "\n",
    "def read_video_pyav(container, indices):\n",
    "    '''\n",
    "    Decode the video with PyAV decoder.\n",
    "    Args:\n",
    "        container (`av.container.input.InputContainer`): PyAV container.\n",
    "        indices (`List[int]`): List of frame indices to decode.\n",
    "    Returns:\n",
    "        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n",
    "    '''\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    start_index = indices[0]\n",
    "    end_index = indices[-1]\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        if i > end_index:\n",
    "            break\n",
    "        if i >= start_index and i in indices:\n",
    "            frames.append(frame)\n",
    "\n",
    "    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
    "\n",
    "def sample_frame_indices(clip_len, frame_sample_rate, seg_len, end_idx):\n",
    "    '''\n",
    "    Sample a given number of frame indices from the video.\n",
    "    Args:\n",
    "        clip_len (`int`): Total number of frames to sample.\n",
    "        frame_sample_rate (`int`): Sample every n-th frame.\n",
    "        seg_len (`int`): Maximum allowed index of sample's last frame. \n",
    "        end_idx (`int`): Last index considered\n",
    "\n",
    "    Returns:\n",
    "        indices (`List[int]`): List of sampled frame indices\n",
    "    '''\n",
    "    converted_len = int(clip_len * frame_sample_rate)\n",
    "    print(\"converted\", converted_len)\n",
    "    print(\"seg\", seg_len)\n",
    "\n",
    "    #end_idx = np.random.randint(converted_len, seg_len)\n",
    "\n",
    "    start_idx = end_idx - converted_len\n",
    "    print(\"start: \", start_idx)\n",
    "    print(\"end: \", end_idx)\n",
    "\n",
    "    indices = np.linspace(start_idx, end_idx, num=clip_len)\n",
    "    indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n",
    "\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup VideoReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decord notes:\n",
    "# https://github.com/dmlc/decord?tab=readme-ov-file            aarch64: build from source to avoid shambles\n",
    "# $ cd decord/python && pip install .                          \n",
    "\n",
    "# https://github.com/huggingface/transformers/issues/21054     CUDA GPU support for inference + XClip issue\n",
    "# https://github.com/huggingface/datasets/issues/5225          dataset video support\n",
    "from decord import VideoReader, cpu\n",
    "\n",
    "videoreader = VideoReader(OUTPUT_FILE, num_threads=1, ctx=cpu(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frame sample size\n",
    "SAMPLE_SIZE = 8\n",
    "FRAME_SAMPLE_RATE = 1\n",
    "\n",
    "container = av.open(OUTPUT_FILE)\n",
    "SEGMENT_LENGTH = container.streams.video[0].frames\n",
    "\n",
    "print(SEGMENT_LENGTH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup frame indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_fps = videoreader.get_avg_fps()\n",
    "\n",
    "for i in range(0, (SEGMENT_LENGTH // SAMPLE_SIZE)):\n",
    "    start_frame = i * SAMPLE_SIZE\n",
    "    end_frame = (i * SAMPLE_SIZE) + SAMPLE_SIZE\n",
    "    start_time = round(start_frame / avg_fps, 2)\n",
    "    end_time = round(end_frame / avg_fps, 2)\n",
    "\n",
    "\n",
    "    df.loc[len(df)] = [start_frame, end_frame, start_time, end_time]\n",
    "\n",
    "# Account for any clipping\n",
    "last_value = df['end_frame'].iloc[-1]\n",
    "df.loc[len(df)] = [last_value, SEGMENT_LENGTH-1, round(last_value / avg_fps), round(SEGMENT_LENGTH-1 / avg_fps)]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video(row):\n",
    "    return videoreader.get_batch(sample_frame_indices(clip_len=SAMPLE_SIZE, frame_sample_rate=FRAME_SAMPLE_RATE, seg_len=container.streams.video[0].frames, end_idx=row['end_frame'])).asnumpy()\n",
    "\n",
    "df['video'] = df.apply(get_video, axis=1)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XClip Model\n",
    "\n",
    "XClip takes a list of text and determines which go best with the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XCLIPProcessor, XCLIPModel\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/xclip-base-patch32\")\n",
    "model = AutoModel.from_pretrained(\"microsoft/xclip-base-patch32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts - essentially a lookup table of visual interests.\n",
    "# if a input passes a specific threshhold, assign greater weight/importance to the frame buffer\n",
    "# for clipping\n",
    "\n",
    "# TODO consider a pipeline that allows user input to be sent to a LLM to prompt and create this list\n",
    "'''\n",
    "interesting_moments = [\n",
    "    \"A surprising or unexpected answer\",\n",
    "    \"A candid or emotional confession\",\n",
    "    \"A witty one-liner or humorous remark\",\n",
    "    \"A memorable quote or sound bite\",\n",
    "    \"A dramatic pause or intense gaze\",\n",
    "    \"A revealing or insightful comment about their craft\",\n",
    "    \"A heartfelt or inspiring message to fans\",\n",
    "    \"A surprising admission or revelation\",\n",
    "    \"A passionate defense of a particular issue\",\n",
    "    \"A unique or quirky habit or ritual\",\n",
    "    \"A fascinating story from their personal life\",\n",
    "    \"A memorable moment when they first got into the industry\",\n",
    "    \"A prediction or forecast for future events\",\n",
    "    \"A candid critique of themselves or others\",\n",
    "    \"A moving tribute to a mentor or idol\",\n",
    "    \"A surprising revelation about their own strengths or weaknesses\",\n",
    "]\n",
    "'''\n",
    "\n",
    "video_labels = [\n",
    "    \"Highlight Worthy\",\n",
    "    \"Neutral\",\n",
    "    \"Not Highlight Worthy\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# TODO MAJOR perf bottleneck right here, batch this with torch dataset loader or something\n",
    "def get_classification(row):\n",
    "    video = numpy.array(row['video']) # perf improvement \n",
    "    inputs = processor(text=video_labels, videos=list(video), return_tensors=\"pt\", padding=True)\n",
    "    # forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    probs = outputs.logits_per_video.softmax(dim=1)\n",
    "    _, predicted = torch.max(probs, dim=1)\n",
    "\n",
    "    return video_labels[predicted.item()]\n",
    "\n",
    "df['vision_classification'] = df.apply(get_classification, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop video bc its huge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['video'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(OUTPUT_DIR + 'out_vision_preprocessing.csv', index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
