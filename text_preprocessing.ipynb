{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "### === Helpers === ###\n",
    "def perror(msg):\n",
    "    print(\"error: \" + msg)\n",
    "\n",
    "def touch(path):\n",
    "    with open(path, 'a') as f:\n",
    "        os.utime(path, None) # set access and modified times\n",
    "        f.close()\n",
    "\n",
    "def mkdir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        return path\n",
    "\n",
    "    print(\"path already exists\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO DYNAMIC PATH HANDLE\n",
    "INPUT_FILE = \"test1.mp4\"\n",
    "\n",
    "INPUT_DIR = './input_data'\n",
    "OUTPUT_DIR = './cache/' # target output for preoprocessing is cache\n",
    "\n",
    "root, extention  = os.path.splitext(INPUT_FILE)\n",
    "OUTPUT_FILE = OUTPUT_DIR + root + \".json\"\n",
    "\n",
    "mkdir(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper_timestamped as whisper\n",
    "import json\n",
    "\n",
    "def get_transcript(input_file, output_file):\n",
    "    audio = whisper.load_audio(input_file)\n",
    "    model = whisper.load_model(\"base\")\n",
    "\n",
    "    result = whisper.transcribe(model, audio, language=\"en\")\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        json.dump(result, file, indent=2, ensure_ascii=False)\n",
    "\n",
    "    return result\n",
    "\n",
    "# Generate transcript if it does not exist in cache\n",
    "if not os.path.isfile(OUTPUT_FILE):\n",
    "    try:\n",
    "        transcript = get_transcript(INPUT_FILE, OUTPUT_FILE)\n",
    "    except:\n",
    "        perror(\"unable to generate transcript\")\n",
    "else: \n",
    "    try:\n",
    "        with open(OUTPUT_FILE, 'r') as f:\n",
    "            transcript = f.read()   \n",
    "            transcript = json.loads(transcript)\n",
    "    except:\n",
    "        perror(\"unable to load data from cache\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(transcript)\n",
    "\n",
    "def get_text(row):\n",
    "    return row['segments']['text']\n",
    "\n",
    "def get_length(row):\n",
    "    return len(row['segments']['text'])\n",
    "\n",
    "def get_start(row):\n",
    "    return row['segments']['start']\n",
    "\n",
    "def get_end(row):\n",
    "    return row['segments']['end']\n",
    "\n",
    "def get_duration(row):\n",
    "    return round(abs(row['segments']['end'] - row['segments']['start']), 2)\n",
    "\n",
    "df['text'] = df.apply(get_text,axis=1)\n",
    "df['text_len'] = df.apply(get_length,axis=1)\n",
    "df['start'] = df.apply(get_start,axis=1)\n",
    "df['end'] = df.apply(get_end,axis=1)\n",
    "df['duration'] = df.apply(get_duration,axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model: [SamLowe/roberta-base-go_emotions](https://huggingface.co/SamLowe/roberta-base-go_emotions) \n",
    "\n",
    "[ONNX Variant](https://huggingface.co/SamLowe/roberta-base-go_emotions-onnx)\n",
    "\n",
    "Model trained from [roberta-base](https://huggingface.co/roberta-base) on the [go_emotions](https://huggingface.co/datasets/go_emotions) dataset for multi-label classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "sentiment_pipe = pipeline(task=\"text-classification\", model=\"SamLowe/roberta-base-go_emotions\", top_k=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add sentiment analysis to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(row):\n",
    "    return sentiment_pipe(row['text'])[0] # produces a list of dicts for each label\n",
    "\n",
    "df['sentiment'] = df.apply(get_sentiment, axis=1)\n",
    "\n",
    "def unpack_sentiment(row):\n",
    "    sentiment = row['sentiment']\n",
    "    if sentiment:\n",
    "        for label_dict in sentiment:\n",
    "            label = label_dict['label'] + \"_sentiment\"\n",
    "            score = label_dict['score']\n",
    "            row[label] = score\n",
    "    return row\n",
    "\n",
    "df = df.apply(unpack_sentiment, axis=1)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Run this:\n",
    "# $ python3 -m spacy download en\n",
    "#nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://catalog.ldc.upenn.edu/docs/LDC2013T19/OntoNotes-Release-5.0.pdf\n",
    "labels_spacy = \"\"\"\n",
    "PERSON                        People, including fictional\n",
    "NORP                          Nationalities or religious or political groups\n",
    "FACILITY                      Buildings, airports, highways, bridges, etc.\n",
    "ORGANIZATION                  Companies, agencies, institutions, etc.\n",
    "GPE                           Countries, cities, states\n",
    "LOCATION                      Non-GPE locations, mountain ranges, bodies of water\n",
    "PRODUCT                       Vehicles, weapons, foods, etc. (Not services)\n",
    "EVENT                         Named hurricanes, battles, wars, sports events, etc.\n",
    "WORK OF ART                   Titles of books, songs, etc.\n",
    "LAW                           Named documents made into laws\n",
    "LANGUAGE                      Any named language\n",
    "DATE                          Absolute or relative dates or periods\n",
    "TIME                          Times smaller than a day\n",
    "PERCENT                       Percentage (including “%”)\n",
    "MONEY                         Monetary values, including unit\n",
    "QUANTITY                      Measurements, as of weight or distance\n",
    "ORDINAL                       “first”, “second”\n",
    "CARDINAL                      Numerals that do not fall under another type\n",
    "\"\"\"\n",
    "\n",
    "label_lookup_table = {\n",
    "    \"PERSON\": \"People, including fictional\",\n",
    "    \"NORP\": \"Nationalities or religious or political groups\",\n",
    "    \"FACILITY\": \"Buildings, airports, highways, bridges, etc.\",\n",
    "    \"ORGANIZATION\": \"Companies, agencies, institutions, etc.\",\n",
    "    \"GPE\": \"Countries, cities, states\",\n",
    "    \"LOCATION\": \"Non-GPE locations, mountain ranges, bodies of water\",\n",
    "    \"PRODUCT\": \"Vehicles, weapons, foods, etc. (Not services)\",\n",
    "    \"EVENT\": \"Named hurricanes, battles, wars, sports events, etc.\",\n",
    "    \"WORK OF ART\": \"Titles of books, songs, etc.\",\n",
    "    \"LAW\": \"Named documents made into laws\",\n",
    "    \"LANGUAGE\": \"Any named language\",\n",
    "    \"DATE\": \"Absolute or relative dates or periods\",\n",
    "    \"TIME\": \"Times smaller than a day\",\n",
    "    \"PERCENT\": \"Percentage (including “%”)\",\n",
    "    \"MONEY\": \"Monetary values, including unit\",\n",
    "    \"QUANTITY\": \"Measurements, as of weight or distance\",\n",
    "    \"ORDINAL\": \"“first”, “second”\",\n",
    "    \"CARDINAL\": \"Numerals that do not fall under another type\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add entities to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO design decision: handling empty columns, keep or toss\n",
    "\n",
    "def get_entity_values(row):\n",
    "    doc = nlp(row['text'])\n",
    "    # Extract entity details and return as a list of tuples\n",
    "    return [(ent.text, ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
    "\n",
    "df['entities'] = df.apply(get_entity_values, axis=1)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop language and segments\n",
    "df = df.drop(columns='language')\n",
    "df = df.drop(columns='segments')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(OUTPUT_DIR + 'out_text_preprocessing.csv', index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note: summary might be something that is irrelevent until clipped at the end of the pipeline?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
